{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "from transformers import GPT2Model\n",
    "\n",
    "# env\n",
    "import gym\n",
    "import slimevolleygym\n",
    "from slimevolleygym import FrameStack\n",
    "from stable_baselines3.common.atari_wrappers import ClipRewardEnv, NoopResetEnv, MaxAndSkipEnv, WarpFrame, ClipRewardEnv\n",
    "\n",
    "# logging and vis\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from time import sleep\n",
    "from gym.envs.classic_control import rendering as rendering\n",
    "from array2gif import write_gif\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-operations",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPT(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            model_name='gpt2',\n",
    "            pretrained=False,\n",
    "            return_last_only=True,\n",
    "            use_embeddings_for_in=False,\n",
    "            in_layer_sizes=None,\n",
    "            out_layer_sizes=None,\n",
    "            freeze_trans=True,\n",
    "            freeze_in=False,\n",
    "            freeze_pos=False,\n",
    "            freeze_ln=False,\n",
    "            freeze_attn=True,\n",
    "            freeze_ff=True,\n",
    "            freeze_out=False,\n",
    "            dropout=0.1,\n",
    "            orth_gain=1.41,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.model_name = model_name\n",
    "        self.return_last_only = return_last_only\n",
    "        self.use_embeddings_for_in = use_embeddings_for_in\n",
    "\n",
    "        self.in_layer_sizes = [] if in_layer_sizes is None else in_layer_sizes\n",
    "        self.out_layer_sizes = [] if out_layer_sizes is None else out_layer_sizes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        if 'gpt' in model_name:\n",
    "            assert model_name in ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "\n",
    "            from transformers import GPT2Model\n",
    "\n",
    "            pretrained_transformer = GPT2Model.from_pretrained(model_name)\n",
    "            if pretrained:\n",
    "                self.transformer = pretrained_transformer\n",
    "            else:\n",
    "                self.transformer = GPT2Model(pretrained_transformer.config)\n",
    "\n",
    "            if model_name == 'gpt2':\n",
    "                embedding_size = 768\n",
    "            elif model_name == 'gpt2-medium':\n",
    "                embedding_size = 1024\n",
    "            elif model_name == 'gpt2-large':\n",
    "                embedding_size = 1280\n",
    "            elif model_name == 'gpt2-xl':\n",
    "                embedding_size = 1600\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('model_name not implemented')\n",
    "\n",
    "        if use_embeddings_for_in:\n",
    "            self.in_net = nn.Embedding(input_dim, embedding_size)\n",
    "        else:\n",
    "            in_layers = []\n",
    "            last_output_size = input_dim\n",
    "            for size in self.in_layer_sizes:\n",
    "                layer = nn.Linear(last_output_size, size)\n",
    "                if orth_gain is not None:\n",
    "                    torch.nn.init.orthogonal_(layer.weight, gain=orth_gain)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "                in_layers.append(layer)\n",
    "                in_layers.append(nn.ReLU())\n",
    "                in_layers.append(nn.Dropout(dropout))\n",
    "                last_output_size = size\n",
    "\n",
    "            final_linear = nn.Linear(last_output_size, embedding_size)\n",
    "            if orth_gain is not None:\n",
    "                torch.nn.init.orthogonal_(final_linear.weight, gain=orth_gain)\n",
    "            final_linear.bias.data.zero_()\n",
    "\n",
    "            in_layers.append(final_linear)\n",
    "            in_layers.append(nn.Dropout(dropout))\n",
    "\n",
    "            self.in_net = nn.Sequential(*in_layers)\n",
    "\n",
    "        out_layers = []\n",
    "        last_output_size = embedding_size\n",
    "        for size in self.out_layer_sizes:\n",
    "            out_layers.append(nn.Linear(last_output_size, size))\n",
    "            out_layers.append(nn.ReLU())\n",
    "            out_layers.append(nn.Dropout(dropout))\n",
    "            last_output_size = size\n",
    "        out_layers.append(nn.Linear(last_output_size, output_dim))\n",
    "        self.out_net = nn.Sequential(*out_layers)\n",
    "\n",
    "        if freeze_trans:\n",
    "            for name, p in self.transformer.named_parameters():\n",
    "                name = name.lower()\n",
    "                if 'ln' in name:\n",
    "                    p.requires_grad = not freeze_ln\n",
    "                elif 'wpe' in name:\n",
    "                    p.requires_grad = not freeze_pos\n",
    "                elif 'mlp' in name:\n",
    "                    p.requires_grad = not freeze_ff\n",
    "                elif 'attn' in name:\n",
    "                    p.requires_grad = not freeze_attn\n",
    "                else:\n",
    "                    p.requires_grad = False\n",
    "        if freeze_in:\n",
    "            for p in self.in_net.parameters():\n",
    "                p.requires_grad = False\n",
    "        if freeze_out:\n",
    "            for p in self.out_net.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "\n",
    "        orig_dim = x.shape[-1]\n",
    "        if orig_dim != self.input_dim and not self.use_embeddings_for_in:\n",
    "            if orig_dim % self.input_dim != 0:\n",
    "                raise ValueError('dimension of x must be divisible by patch size')\n",
    "            ratio = orig_dim // self.input_dim\n",
    "            x = x.reshape(x.shape[0], x.shape[1] * ratio, self.input_dim)\n",
    "        else:\n",
    "            ratio = 1\n",
    "\n",
    "        x = self.in_net(x)\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            inputs_embeds=x,\n",
    "            return_dict=True,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        x = transformer_outputs.last_hidden_state\n",
    "\n",
    "        if self.return_last_only:\n",
    "            x = x[:,-ratio:]\n",
    "\n",
    "        x = self.out_net(x)\n",
    "        if self.return_last_only and ratio > 1:\n",
    "            x = x.reshape(x.shape[0], x.shape[1] // ratio, ratio * self.output_dim)\n",
    "\n",
    "        if output_attentions:\n",
    "            return x, transformer_outputs.attentions\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-liability",
   "metadata": {},
   "source": [
    "## GPT Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ortho_init(module, gain):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.orthogonal_(module.weight, gain=gain)\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.fill_(0.0) \n",
    "\n",
    "\n",
    "class GPTActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 out_dim,\n",
    "                 n_actions,\n",
    "                 patch_size,\n",
    "                 device='cuda',\n",
    "                 model_name='gpt2',\n",
    "                 pretrained=False,\n",
    "                 return_last_only=True,\n",
    "                 use_embeddings_for_in=False,\n",
    "                 in_layer_sizes=None,\n",
    "                 out_layer_sizes=None,\n",
    "                 freeze_trans=True,\n",
    "                 freeze_in=False,\n",
    "                 freeze_pos=False,\n",
    "                 freeze_ln=False,\n",
    "                 freeze_attn=True,\n",
    "                 freeze_ff=True,\n",
    "                 freeze_out=False,\n",
    "                 dropout=0.1,\n",
    "                 orth_gain=1.41):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.fpt = FPT(input_dim,\n",
    "                       out_dim,\n",
    "                       model_name,\n",
    "                       pretrained,\n",
    "                       return_last_only,\n",
    "                       use_embeddings_for_in,\n",
    "                       in_layer_sizes,\n",
    "                       out_layer_sizes,\n",
    "                       freeze_trans,\n",
    "                       freeze_in,\n",
    "                       freeze_pos,\n",
    "                       freeze_ln,\n",
    "                       freeze_attn,\n",
    "                       freeze_ff,\n",
    "                       freeze_out,\n",
    "                       dropout,\n",
    "                       orth_gain)\n",
    "        \n",
    "        self.policy_head = nn.Linear(out_dim, n_actions) #TODO output dim\n",
    "        self.value_head  = nn.Linear(out_dim, 1)\n",
    "        \n",
    "        self.init_head_weights()\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        obs = self.prepare_obs(obs) # (bs, n_patches, features_dim)\n",
    "    \n",
    "        features = self.fpt(obs).squeeze(1) # (bs, features)\n",
    "        policy = Categorical(logits=self.policy_head(features))\n",
    "        value  = self.value_head(features)\n",
    "        return policy, value\n",
    "        \n",
    "        \n",
    "    def prepare_obs(self, obs):\n",
    "        \"\"\"\n",
    "        prepares numpy observations for GPT feature extractor\n",
    "        input: np.array (bs, h, w, c)\n",
    "        returns: torch.tensor (bs, n_patches, features_dim)\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(obs):\n",
    "            # if True, obs comes from environment. Else, from Rollout Buffer \n",
    "            obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "            obs = obs / 255.\n",
    "        \n",
    "        # channel first\n",
    "        obs = rearrange(obs, 'b h w c -> b c h w')\n",
    "        # tokenise\n",
    "        obs = rearrange(obs, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n",
    "        return obs.to(self.device)\n",
    "    \n",
    "    def init_head_weights(self):\n",
    "        \"\"\"\n",
    "        Orthogonal initialization for policy and value head with OpenAI baseline gains\n",
    "        \"\"\"\n",
    "        module_gains = {\n",
    "                self.policy_head: 0.01,\n",
    "                self.value_head: 1,\n",
    "        }\n",
    "        for module, gain in module_gains.items():\n",
    "            module.apply(partial(ortho_init, gain=gain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-smooth",
   "metadata": {},
   "source": [
    "## Slimevolley Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed):\n",
    "    # almost the same as typical Atari processing for CNN agent.\n",
    "    env = gym.make(\"SlimeVolleySurvivalNoFrameskip-v0\")\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    env = WarpFrame(env)\n",
    "    #env = ClipRewardEnv(env)\n",
    "    env = FrameStack(env, 4)\n",
    "    env.seed(seed)\n",
    "    return env\n",
    "\n",
    "\n",
    "def make_eval_env(seed):\n",
    "    env = gym.make(\"SlimeVolleyNoFrameskip-v0\")\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    env = WarpFrame(env)\n",
    "    #env = ClipRewardEnv(env)\n",
    "    env = FrameStack(env, 4)\n",
    "    env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-district",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-uganda",
   "metadata": {},
   "source": [
    "### Rolloutbuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "RolloutBufferSamples = namedtuple('RolloutBufferSamples', ('observations', 'actions', 'old_values', 'old_log_probs', 'advantages', 'returns'))\n",
    "\n",
    "class RolloutBuffer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size,\n",
    "        observation_size,\n",
    "        device = \"cpu\",\n",
    "        gae_lambda = 1,\n",
    "        gamma = 0.99\n",
    "    ):  \n",
    "        \n",
    "        self.buffer_size = buffer_size\n",
    "        self.observation_size = observation_size\n",
    "        \n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.pos = 0\n",
    "        self.full = False\n",
    "        self.generator_ready = False\n",
    "        \n",
    "        zeros = lambda shape: np.zeros(shape, dtype=np.float32)\n",
    "        self.observations = zeros((self.buffer_size,) + self.observation_size)\n",
    "        self.actions      = zeros((self.buffer_size, 1))\n",
    "        self.rewards      = zeros((self.buffer_size, 1))\n",
    "        self.returns      = zeros((self.buffer_size, 1))\n",
    "        self.dones        = zeros((self.buffer_size, 1))\n",
    "        self.values       = zeros((self.buffer_size, 1))\n",
    "        self.log_probs    = zeros((self.buffer_size, 1))\n",
    "        self.advantages   = zeros((self.buffer_size, 1))\n",
    "\n",
    "    def compute_returns_and_advantage(self, last_values, dones):\n",
    "        \n",
    "        last_values = last_values.clone().cpu().numpy().flatten()\n",
    "        last_gae_lam = 0\n",
    "        \n",
    "        for step in reversed(range(self.buffer_size)):\n",
    "            if step == self.buffer_size - 1:\n",
    "                next_non_terminal = 1.0 - dones\n",
    "                next_values = last_values\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - self.dones[step + 1]\n",
    "                next_values = self.values[step + 1]\n",
    "            delta = self.rewards[step] + self.gamma * next_values * next_non_terminal - self.values[step]\n",
    "            last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam\n",
    "            self.advantages[step] = last_gae_lam\n",
    "        self.returns = self.advantages + self.values\n",
    "\n",
    "    def add(self, obs, action, reward, done, value, log_prob):\n",
    "    \n",
    "        if len(log_prob.shape) == 0:\n",
    "            log_prob = log_prob.reshape(-1, 1)\n",
    "        \n",
    "        self.observations[self.pos] = np.array(obs).copy()\n",
    "        self.actions[self.pos]      = np.array(action).copy()\n",
    "        self.rewards[self.pos]      = np.array(reward).copy()\n",
    "        self.dones[self.pos]        = np.array(done).copy()\n",
    "        self.values[self.pos]       = value.clone().cpu().numpy().flatten()\n",
    "        self.log_probs[self.pos]    = log_prob.clone().cpu().numpy()\n",
    "        \n",
    "        self.pos += 1\n",
    "        \n",
    "        if self.pos == self.buffer_size:\n",
    "            self.full = True\n",
    "\n",
    "    def get(self, batch_size):\n",
    "        \n",
    "        indices = np.random.permutation(self.buffer_size)\n",
    "        start_idx = 0\n",
    "        while start_idx < self.buffer_size:\n",
    "            yield self._get_samples(indices[start_idx : start_idx + batch_size])\n",
    "            start_idx += batch_size\n",
    "\n",
    "    def _get_samples(self, batch_inds):\n",
    "        data = (\n",
    "            self.observations[batch_inds],\n",
    "            self.actions[batch_inds],\n",
    "            self.values[batch_inds].flatten(),\n",
    "            self.log_probs[batch_inds].flatten(),\n",
    "            self.advantages[batch_inds].flatten(),\n",
    "            self.returns[batch_inds].flatten(),\n",
    "        )\n",
    "        \n",
    "        return RolloutBufferSamples(*tuple(map(self.to_torch, data)))\n",
    "    \n",
    "    def to_torch(self, array):\n",
    "        return torch.tensor(array).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explained_variance(y_pred, y_true):\n",
    "    assert y_true.ndim == 1 and y_pred.ndim == 1\n",
    "    var_y = np.var(y_true)\n",
    "    return np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-program",
   "metadata": {},
   "source": [
    "### Evaluation and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_atari(obs):\n",
    "    \"\"\"\n",
    "    Helper function that takes in a processed obs (84,84,4)\n",
    "    Useful for visualizing what an Atari agent actually *sees*\n",
    "    Outputs in Atari visual format (Top: resized to orig dimensions, buttom: 4 frames)\n",
    "    \"\"\"\n",
    "    tempObs = []\n",
    "    obs = np.copy(obs)\n",
    "    for i in range(4):\n",
    "        if i == 3:\n",
    "            latest = np.copy(obs[:, :, i])\n",
    "        if i > 0: # insert vertical lines\n",
    "            obs[:, 0, i] = 141\n",
    "        tempObs.append(obs[:, :, i])\n",
    "    latest = np.expand_dims(latest, axis=2)\n",
    "    latest = np.concatenate([latest*255.0] * 3, axis=2).astype(np.uint8)\n",
    "    latest = cv2.resize(latest, (84 * 8, 84 * 4), interpolation=cv2.INTER_NEAREST)\n",
    "    tempObs = np.concatenate(tempObs, axis=1)\n",
    "    tempObs = np.expand_dims(tempObs, axis=2)\n",
    "    tempObs = np.concatenate([tempObs*255.0] * 3, axis=2).astype(np.uint8)\n",
    "    tempObs = cv2.resize(tempObs, (84 * 8, 84 * 2), interpolation=cv2.INTER_NEAREST)\n",
    "    return np.concatenate([latest, tempObs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_agent(agent, env, n_episodes=100, render=False, gif_path=None):\n",
    "    echo(f'Evaluating Agent...')\n",
    "    \n",
    "    total_rewards = []\n",
    "    all_frames = []\n",
    "    \n",
    "    if render:\n",
    "        viewer = rendering.SimpleImageViewer(maxwidth=2160)\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        \n",
    "        rollout_frames = []\n",
    "        \n",
    "        obs  = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            policy, value = agent(obs[None])\n",
    "            #action = policy.logits.argmax(dim=1) # exploit at test time\n",
    "            action = policy.sample()\n",
    "            \n",
    "            action = action.cpu().numpy()\n",
    "            obs, reward, done, _ = env.step(action.item())\n",
    "            \n",
    "            if render:\n",
    "                viewer.imshow(render_atari(obs))\n",
    "                sleep(0.08)\n",
    "                \n",
    "            if gif_path is not None:\n",
    "                frame = np.einsum('hwc->chw', render_atari(obs))\n",
    "                rollout_frames.append(frame)\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "        total_rewards.append(total_reward)\n",
    "        all_frames.append(rollout_frames)\n",
    "        echo(f'Episode {i+1} done.')\n",
    "        \n",
    "    if render:\n",
    "        viewer.close()\n",
    "        \n",
    "    if gif_path is not None:\n",
    "        print(f'Writing gifs to {gif_path}...')\n",
    "        gif_path = Path(gif_path)\n",
    "        if not gif_path.exists(): \n",
    "            gif_path.mkdir()\n",
    "        for i, frames in enumerate(all_frames, start=1):\n",
    "            write_gif(frames, gif_path/f'rollout_{i}.gif', fps=5)\n",
    "            echo(f'Rollout {i}/{n_episodes} done.')\n",
    "    \n",
    "    return np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-monroe",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "def set_random_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def echo(message):\n",
    "    print(message + ' '*1000, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "## General\n",
    "SEED = 101\n",
    "#DEVICE = torch.device('cuda:0')\n",
    "DEVICE = torch.device('cuda:0')\n",
    "MAX_STEPS = 1_000_000\n",
    "EVAL_FREQ = 10_000\n",
    "EVAL_EPISODES = 100 # make sure to rerun on 1000 episodes afterwards to measure that performance, don't use only 100.\n",
    "\n",
    "## PPO\n",
    "LEARNING_RATE = 3e-4\n",
    "MAX_GRAD_NORM = 0.5\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.0\n",
    "TARGET_KL = None\n",
    "BATCH_SIZE = 12\n",
    "PPO_CLIP_RANGE = 0.2\n",
    "BUFFER_SIZE = 4096 # 2048\n",
    "PPO_EPOCHS = 4 #10\n",
    "DISCOUNT = 0.99\n",
    "TRACE_DECAY = 0.95\n",
    "\n",
    "## GPT Actor Critic\n",
    "PATCH_SIZE = 4\n",
    "INPUT_SIZE = 4 * PATCH_SIZE**2\n",
    "OUT_SIZE = 64\n",
    "\n",
    "## LOGGING\n",
    "LOGGING_MODE = 'online'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init env\n",
    "set_random_seeds(SEED)\n",
    "env = make_env(SEED)\n",
    "eval_env = make_eval_env(SEED)\n",
    "\n",
    "# init model\n",
    "agent = GPTActorCritic(input_dim=INPUT_SIZE, \n",
    "                       out_dim=OUT_SIZE,\n",
    "                       n_actions=env.action_space.n,\n",
    "                       patch_size=PATCH_SIZE,\n",
    "                       device=DEVICE,\n",
    "                       pretrained=True,\n",
    "                       freeze_trans=True,\n",
    "                       freeze_in=False,\n",
    "                       freeze_pos=False,\n",
    "                       freeze_ln=False,\n",
    "                       freeze_attn=True,\n",
    "                       freeze_ff=True,\n",
    "                       freeze_out=False) \n",
    "\n",
    "agent = agent.to(DEVICE)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# init rollout buffer\n",
    "rollout_buffer = RolloutBuffer(BUFFER_SIZE, env.observation_space.shape, gamma=DISCOUNT, gae_lambda=TRACE_DECAY, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-coordinator",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with wandb.init(project=\"rl-gpt\", mode=LOGGING_MODE):\n",
    "\n",
    "    last_obs  = env.reset()\n",
    "    last_done = False\n",
    "    total_reward = 0.\n",
    "\n",
    "    pbar = tqdm(range(MAX_STEPS), unit_scale=1, smoothing=0)\n",
    "    for step in pbar:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #obs_tensor = torch.as_tensor(last_obs).to(DEVICE)\n",
    "            policy, value = agent(last_obs[None])\n",
    "            action = policy.sample()\n",
    "            log_prob = policy.log_prob(action)\n",
    "\n",
    "        #import pdb; pdb.set_trace()\n",
    "        action = action.cpu().numpy()\n",
    "        obs, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        rollout_buffer.add(last_obs, action, reward, last_done, value, log_prob)\n",
    "\n",
    "        last_obs = obs\n",
    "        last_done = done\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            last_obs = env.reset()\n",
    "            pbar.set_description('Step: %i | Reward: %f' % (step, total_reward))\n",
    "            wandb.log({'total_reward': total_reward})\n",
    "            total_reward = 0.\n",
    "            \n",
    "        # evaluate policy\n",
    "        if step % EVAL_FREQ == 0:\n",
    "            agent.eval()\n",
    "            mean_eval_reward = evaluate_agent(agent, eval_env, EVAL_EPISODES)\n",
    "            wandb.log({'mean_eval_reward': mean_eval_reward})\n",
    "            \n",
    "            # early stopping\n",
    "            if mean_eval_reward >= 0.0:\n",
    "                echo(f'Mean Eval Reward: {mean_eval_reward}. Early stopping...')\n",
    "                break\n",
    "            \n",
    "        \n",
    "        # train policy\n",
    "        if rollout_buffer.full:\n",
    "            agent.train()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Compute value for the last timestep\n",
    "                #obs_tensor = torch.as_tensor(obs).to(DEVICE)\n",
    "                _, value = agent(obs[None])\n",
    "\n",
    "            # compute returns and advantages\n",
    "            rollout_buffer.compute_returns_and_advantage(value, done)\n",
    "\n",
    "            clip_range = PPO_CLIP_RANGE\n",
    "\n",
    "            # containers for collecting training stats\n",
    "            entropy_losses = [] \n",
    "            all_kl_divs    = []\n",
    "            policy_losses  = []\n",
    "            value_losses   = []\n",
    "            clip_fractions = []\n",
    "            approx_kl_divs = []\n",
    "\n",
    "            # Start Training\n",
    "            for epoch in range(PPO_EPOCHS):\n",
    "                echo(f'Step: {step}; Epoch: {epoch}; Starting PPO Training...')\n",
    "                for rollout_data in rollout_buffer.get(BATCH_SIZE):\n",
    "                    actions = rollout_data.actions.long().flatten()\n",
    "\n",
    "                    # evaluate actions\n",
    "                    policy, value = agent(rollout_data.observations)\n",
    "                    log_prob = policy.log_prob(actions)\n",
    "                    entropy = policy.entropy()\n",
    "                    value = value.flatten()\n",
    "\n",
    "                    # Extra step: normalize advantage\n",
    "                    advantages = rollout_data.advantages\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                    # ratio between old and new policy, should be one at the first iteration\n",
    "                    ratio = torch.exp(log_prob - rollout_data.old_log_probs)\n",
    "\n",
    "                    # clipped surrogate loss\n",
    "                    policy_loss_1 = advantages * ratio\n",
    "                    policy_loss_2 = advantages * torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "                    policy_loss   = -torch.min(policy_loss_1, policy_loss_2).mean()\n",
    "\n",
    "                    # Value loss using the TD(gae_lambda) target\n",
    "                    value_loss = F.mse_loss(rollout_data.returns, value)\n",
    "\n",
    "                    entropy_loss = -torch.mean(entropy)\n",
    "\n",
    "                    loss = policy_loss + ENTROPY_COEF * entropy_loss + VALUE_COEF * value_loss\n",
    "\n",
    "                    # Optimization step\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Logging\n",
    "                    approx_kl_divs.append(torch.mean(rollout_data.old_log_probs - log_prob).detach().cpu().numpy())\n",
    "                    policy_losses.append(policy_loss.item())\n",
    "                    clip_fraction = torch.mean((torch.abs(ratio - 1) > clip_range).float()).item()\n",
    "                    clip_fractions.append(clip_fraction)\n",
    "                    value_losses.append(value_loss.item())\n",
    "                    entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "                echo(f'PPO Epoch {epoch} done.')\n",
    "                \n",
    "                all_kl_divs.append(np.mean(approx_kl_divs))\n",
    "\n",
    "                if TARGET_KL is not None and np.mean(approx_kl_divs) > 1.5 * TARGET_KL:\n",
    "                    print(f'Early stopping at step {step} due to reaching max kl: {np.mean(approx_kl_divs):.2f}')\n",
    "                    break\n",
    "\n",
    "            explained_var = explained_variance(rollout_buffer.values.flatten(), rollout_buffer.returns.flatten())\n",
    "\n",
    "            # Empty the rollout buffer\n",
    "            rollout_buffer.reset()\n",
    "\n",
    "            # Log Training Progress\n",
    "            wandb.log({\"entropy_loss\": np.mean(entropy_losses)}, step=step)\n",
    "            wandb.log({\"policy_gradient_loss\": np.mean(policy_losses)}, step=step)\n",
    "            wandb.log({\"value_loss\": np.mean(value_losses)}, step=step)\n",
    "            wandb.log({\"approx_kl\": np.mean(approx_kl_divs)}, step=step)\n",
    "            wandb.log({\"clip_fraction\": np.mean(clip_fractions)}, step=step)\n",
    "            wandb.log({\"loss\": loss.item()}, step=step)\n",
    "            wandb.log({\"explained_variance\": explained_var}, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-lambda",
   "metadata": {},
   "source": [
    "## Visualise Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.eval()\n",
    "evaluate_agent(agent, eval_env, n_episodes=1, render=True, gif_path='./gifs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
